{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41697590-46ff-48dd-b5c4-9ed872412dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'MeZO'...\n",
      "remote: Enumerating objects: 173, done.\u001b[K\n",
      "remote: Counting objects: 100% (75/75), done.\u001b[K\n",
      "remote: Compressing objects: 100% (45/45), done.\u001b[K\n",
      "remote: Total 173 (delta 45), reused 30 (delta 30), pack-reused 98 (from 1)\u001b[K\n",
      "Receiving objects: 100% (173/173), 432.68 KiB | 8.16 MiB/s, done.\n",
      "Resolving deltas: 100% (88/88), done.\n"
     ]
    }
   ],
   "source": [
    "# !git clone https://github.com/princeton-nlp/MeZO.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e3e9fea-5199-4a71-bd6c-621d1d5d5405",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.4.0+cu121)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch) (1.13.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.11/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /opt/conda/lib/python3.11/site-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Collecting transformers==4.28.1\n",
      "  Downloading transformers-4.28.1-py3-none-any.whl.metadata (109 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers==4.28.1) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0 (from transformers==4.28.1)\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers==4.28.1) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers==4.28.1) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers==4.28.1) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.28.1)\n",
      "  Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers==4.28.1) (2.32.3)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.28.1)\n",
      "  Downloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers==4.28.1) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.1) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.1) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers==4.28.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers==4.28.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers==4.28.1) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers==4.28.1) (2024.7.4)\n",
      "Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.7/792.7 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, regex, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.30.2 regex-2024.11.6 tokenizers-0.13.3 transformers-4.28.1\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "E: Unable to locate package jq\n",
      "Collecting loralib\n",
      "  Downloading loralib-0.1.2-py3-none-any.whl.metadata (15 kB)\n",
      "Downloading loralib-0.1.2-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: loralib\n",
      "Successfully installed loralib-0.1.2\n",
      "Collecting nvidia-ml-py3\n",
      "  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: nvidia-ml-py3\n",
      "  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19171 sha256=abc33282441db7aba947759fb74691d9542ebd541fa94b0c15e5df0c01de7ecc\n",
      "  Stored in directory: /home/javad/.cache/pip/wheels/47/50/9e/29dc79037d74c3c1bb4a8661fb608e8674b7e4260d6a3f8f51\n",
      "Successfully built nvidia-ml-py3\n",
      "Installing collected packages: nvidia-ml-py3\n",
      "Successfully installed nvidia-ml-py3-7.352.0\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.11/site-packages (from datasets) (4.66.4)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
      "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Installing collected packages: xxhash, multiprocess, datasets\n",
      "Successfully installed datasets-3.5.0 multiprocess-0.70.16 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "# Install the latest compatible PyTorch version with CUDA 11.8\n",
    "!pip install torch --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install transformers==4.28.1\n",
    "!sudo apt-get install jq\n",
    "!pip install loralib\n",
    "!pip install nvidia-ml-py3\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "944e63a7-8f4c-4277-8d97-4640831f8103",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 http://archive.ubuntu.com/ubuntu jammy InRelease [270 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]        \n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
      "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 Packages [1792 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy/universe amd64 Packages [17.5 MB] \n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy/restricted amd64 Packages [164 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 Packages [266 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1542 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3150 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [55.7 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4266 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [82.7 kB]\n",
      "Get:15 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [47.7 kB]\n",
      "Get:16 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2837 kB]\n",
      "Get:17 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1243 kB]\n",
      "Get:18 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [4104 kB]\n",
      "Fetched 37.7 MB in 5s (8306 kB/s)                          \n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  libjq1 libonig5\n",
      "The following NEW packages will be installed:\n",
      "  jq libjq1 libonig5\n",
      "0 upgraded, 3 newly installed, 0 to remove and 82 not upgraded.\n",
      "Need to get 357 kB of archives.\n",
      "After this operation, 1087 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libonig5 amd64 6.9.7.1-2build1 [172 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjq1 amd64 1.6-2.1ubuntu3 [133 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 jq amd64 1.6-2.1ubuntu3 [52.5 kB]\n",
      "Fetched 357 kB in 0s (3138 kB/s)\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package libonig5:amd64.\n",
      "(Reading database ... 49713 files and directories currently installed.)\n",
      "Preparing to unpack .../libonig5_6.9.7.1-2build1_amd64.deb ...\n",
      "Unpacking libonig5:amd64 (6.9.7.1-2build1) ...\n",
      "Selecting previously unselected package libjq1:amd64.\n",
      "Preparing to unpack .../libjq1_1.6-2.1ubuntu3_amd64.deb ...\n",
      "Unpacking libjq1:amd64 (1.6-2.1ubuntu3) ...\n",
      "Selecting previously unselected package jq.\n",
      "Preparing to unpack .../jq_1.6-2.1ubuntu3_amd64.deb ...\n",
      "Unpacking jq (1.6-2.1ubuntu3) ...\n",
      "Setting up libonig5:amd64 (6.9.7.1-2build1) ...\n",
      "Setting up libjq1:amd64 (1.6-2.1ubuntu3) ...\n",
      "Setting up jq (1.6-2.1ubuntu3) ...\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get update\n",
    "!sudo apt-get install jq -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68ccf936-e3d3-4779-8c6e-4d61c96c9c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/jq\n"
     ]
    }
   ],
   "source": [
    "!which jq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4627227-0e62-47a6-aeb1-c77c6e1d7bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import time\n",
    "import csv\n",
    "from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetPowerUsage, nvmlShutdown\n",
    "import os\n",
    "import subprocess\n",
    "import torch\n",
    "from pynvml import *\n",
    "from threading import Thread\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer, Trainer, TrainingArguments, AutoModel\n",
    "from datasets import load_dataset\n",
    "import torch.nn.utils.prune as prune\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import matplotlib.colors as mcolors\n",
    "import subprocess\n",
    "import glob\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "936d4732-00e5-4d6e-ab94-77a1c524f8f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/javad/seedfl/medium_models/data\n",
      "--2025-04-23 05:51:03--  https://nlp.cs.princeton.edu/projects/lm-bff/datasets.tar\n",
      "Resolving nlp.cs.princeton.edu (nlp.cs.princeton.edu)... 128.112.136.67\n",
      "Connecting to nlp.cs.princeton.edu (nlp.cs.princeton.edu)|128.112.136.67|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1901486080 (1.8G) [application/x-tar]\n",
      "Saving to: ‘datasets.tar’\n",
      "\n",
      "datasets.tar        100%[===================>]   1.77G  22.7MB/s    in 66s     \n",
      "\n",
      "2025-04-23 05:52:10 (27.5 MB/s) - ‘datasets.tar’ saved [1901486080/1901486080]\n",
      "\n",
      "original/\n",
      "original/CoLA/\n",
      "original/CoLA/test.tsv\n",
      "original/CoLA/dev.tsv\n",
      "original/CoLA/train.tsv\n",
      "original/GLUE-SST-2/\n",
      "original/GLUE-SST-2/test.tsv\n",
      "original/GLUE-SST-2/dev.tsv\n",
      "original/GLUE-SST-2/train.tsv\n",
      "original/mr/\n",
      "original/mr/process.py\n",
      "original/mr/mr.all\n",
      "original/mr/train.csv\n",
      "original/mr/test.csv\n",
      "original/trec/\n",
      "original/trec/TREC.test.all\n",
      "original/trec/process.py\n",
      "original/trec/train.csv\n",
      "original/trec/test.csv\n",
      "original/trec/TREC.train.all\n",
      "original/WNLI/\n",
      "original/WNLI/test.tsv\n",
      "original/WNLI/dev.tsv\n",
      "original/WNLI/train.tsv\n",
      "original/SNLI/\n",
      "original/SNLI/test.tsv\n",
      "original/SNLI/original/\n",
      "original/SNLI/original/snli_1.0_dev.jsonl\n",
      "original/SNLI/original/snli_1.0_test.jsonl\n",
      "original/SNLI/original/snli_1.0_train.txt\n",
      "original/SNLI/original/snli_1.0_test.txt\n",
      "original/SNLI/original/snli_1.0_train.jsonl\n",
      "original/SNLI/original/snli_1.0_dev.txt\n",
      "original/SNLI/dev.tsv\n",
      "original/SNLI/README.txt\n",
      "original/SNLI/train.tsv\n",
      "original/mpqa/\n",
      "original/mpqa/process.py\n",
      "original/mpqa/train.csv\n",
      "original/mpqa/test.csv\n",
      "original/mpqa/mpqa.all\n",
      "original/QQP/\n",
      "original/QQP/test.tsv\n",
      "original/QQP/dev.tsv\n",
      "original/QQP/train.tsv\n",
      "original/STS-B/\n",
      "original/STS-B/test.tsv\n",
      "original/STS-B/readme.txt\n",
      "original/STS-B/original/\n",
      "original/STS-B/original/sts-test.tsv\n",
      "original/STS-B/original/sts-dev.tsv\n",
      "original/STS-B/original/sts-train.tsv\n",
      "original/STS-B/LICENSE.txt\n",
      "original/STS-B/dev.tsv\n",
      "original/STS-B/train.tsv\n",
      "original/RTE/\n",
      "original/RTE/test.tsv\n",
      "original/RTE/dev.tsv\n",
      "original/RTE/train.tsv\n",
      "original/subj/\n",
      "original/subj/process.py\n",
      "original/subj/subj.all\n",
      "original/subj/train.csv\n",
      "original/subj/test.csv\n",
      "original/QNLI/\n",
      "original/QNLI/test.tsv\n",
      "original/QNLI/dev.tsv\n",
      "original/QNLI/train.tsv\n",
      "original/MNLI/\n",
      "original/MNLI/test_matched.tsv\n",
      "original/MNLI/test_mismatched.tsv\n",
      "original/MNLI/dev_matched.tsv\n",
      "original/MNLI/README.txt\n",
      "original/MNLI/train.tsv\n",
      "original/MNLI/dev_mismatched.tsv\n",
      "original/cr/\n",
      "original/cr/custrev.all\n",
      "original/cr/process.py\n",
      "original/cr/train.csv\n",
      "original/cr/test.csv\n",
      "original/MRPC/\n",
      "original/MRPC/test.tsv\n",
      "original/MRPC/msr_paraphrase_test.txt\n",
      "original/MRPC/msr_paraphrase_train.txt\n",
      "original/MRPC/dev.tsv\n",
      "original/MRPC/dev_ids.tsv\n",
      "original/MRPC/train.tsv\n",
      "original/SST-2/\n",
      "original/SST-2/test.tsv\n",
      "original/SST-2/transfer.py\n",
      "original/SST-2/dev.tsv\n",
      "original/SST-2/train.tsv\n",
      "original/sst-5/\n",
      "original/sst-5/process.py\n",
      "original/sst-5/stsa.fine.test\n",
      "original/sst-5/stsa.fine.dev\n",
      "original/sst-5/stsa.fine.train\n",
      "original/sst-5/train.csv\n",
      "original/sst-5/test.csv\n",
      "*** Use GLUE-SST-2 as default SST-2 ***\n",
      "*** Done ***\n"
     ]
    }
   ],
   "source": [
    "%cd medium_models/data\n",
    "!bash download_dataset.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed0ebeb3-f051-47a3-bd18-60469e7eca52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/javad/seedfl/medium_models\n",
      "K = 128\n",
      "Seed = 42\n",
      "| Task = SST-2\n"
     ]
    }
   ],
   "source": [
    "%cd ../\n",
    "\n",
    "!python tools/generate_k_shot_data.py --mode k-shot-1k-test --k 128 --task SST-2 --seed 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dac8175-1832-4987-b53d-13a7a0c56f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/k-shot-1k-test/SST-2/128-42/train_part1.tsv\n"
     ]
    }
   ],
   "source": [
    "def split_train_tsv(file_path, num_parts=1, seed=42):\n",
    "    random.seed(seed)\n",
    "\n",
    "    with open(file_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    header = lines[0]\n",
    "    data_lines = lines[1:]\n",
    "\n",
    "    # Group by label\n",
    "    label_dict = {}\n",
    "    for line in data_lines:\n",
    "        label = line.strip().split('\\t')[-1]\n",
    "        if label not in label_dict:\n",
    "            label_dict[label] = []\n",
    "        label_dict[label].append(line)\n",
    "\n",
    "    # Ensure we have only two labels\n",
    "    assert len(label_dict) == 2, f\"Expected binary classification, found labels: {list(label_dict.keys())}\"\n",
    "\n",
    "    # Shuffle each label group\n",
    "    for label in label_dict:\n",
    "        random.shuffle(label_dict[label])\n",
    "\n",
    "    # Calculate min samples per label\n",
    "    min_len = min(len(label_dict[label]) for label in label_dict)\n",
    "    part_len = min_len // num_parts\n",
    "\n",
    "    for i in range(num_parts):\n",
    "        part_lines = []\n",
    "        for label in sorted(label_dict.keys()):\n",
    "            start_idx = i * part_len\n",
    "            end_idx = (i + 1) * part_len\n",
    "            part_lines.extend(label_dict[label][start_idx:end_idx])\n",
    "\n",
    "        # Shuffle mixed part lines before saving\n",
    "        random.shuffle(part_lines)\n",
    "\n",
    "        part_path = file_path.replace(\"train.tsv\", f\"train_part{i+1}.tsv\")\n",
    "        with open(part_path, \"w\") as out:\n",
    "            out.write(header)\n",
    "            for line in part_lines:\n",
    "                out.write(line)\n",
    "\n",
    "        print(f\"Saved: {part_path}\")\n",
    "\n",
    "split_train_tsv(f\"data/k-shot-1k-test/SST-2/128-42/train.tsv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b435116-80b6-4280-9766-6b1bda69514d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_dir):\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(model_dir)\n",
    "    model = RobertaModelForPromptFinetuning.from_pretrained(model_dir)\n",
    "    model.model_args = DynamicTrainingArguments()\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    test_samples = [\n",
    "        {\"text\": \"This movie is great! It was\", \"label\": \"positive\"},\n",
    "        {\"text\": \"I hated it. It was\", \"label\": \"negative\"}\n",
    "    ]\n",
    "    label_words = {\"positive\": \"Ġgreat\", \"negative\": \"Ġterrible\"}\n",
    "    label_ids = {label: tokenizer.convert_tokens_to_ids(word) for label, word in label_words.items()}\n",
    "\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for sample in test_samples:\n",
    "            text_with_mask = f\"{sample['text']} <mask>.\"\n",
    "            inputs = tokenizer(text_with_mask, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "            input_ids = inputs[\"input_ids\"].to(device)\n",
    "            attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "            print(f\"Text: {text_with_mask}\")\n",
    "            print(f\"Input IDs: {input_ids}\")\n",
    "            print(f\"Attention Mask: {attention_mask}\")\n",
    "            mask_pos = torch.where(input_ids[0] == tokenizer.mask_token_id)[0].item()\n",
    "            print(f\"Mask Position: {mask_pos}\")\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, mask_pos=torch.tensor([mask_pos]).to(device))\n",
    "            logits = outputs[0]\n",
    "            print(f\"Logits Shape: {logits.shape}\")\n",
    "\n",
    "            if len(logits.shape) == 2 and logits.shape[1] == 2:\n",
    "                roberta_outputs = model.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                hidden_states = roberta_outputs[0]\n",
    "                if hasattr(model, 'lm_head'):\n",
    "                    logits = model.lm_head(hidden_states)\n",
    "                else:\n",
    "                    logits = model.classifier(hidden_states)\n",
    "                    if logits.shape[-1] == 2:\n",
    "                        raise ValueError(\"Model outputs classification logits instead of token logits. Missing LM head?\")\n",
    "                print(f\"Corrected Logits Shape: {logits.shape}\")\n",
    "\n",
    "            logits_at_mask = logits[0, mask_pos]\n",
    "            prob_great = logits_at_mask[label_ids[\"positive\"]].item()\n",
    "            prob_terrible = logits_at_mask[label_ids[\"negative\"]].item()\n",
    "            pred_label = \"positive\" if prob_great > prob_terrible else \"negative\"\n",
    "\n",
    "            print(f\"Predicted: {pred_label}, Expected: {sample['label']}\")\n",
    "            print(f\"Logits - great: {prob_great}, terrible: {prob_terrible}\")\n",
    "            correct += (pred_label == sample[\"label\"])\n",
    "\n",
    "    accuracy = correct / len(test_samples) * 100\n",
    "    print(f\"Evaluation accuracy for {model_dir}: {accuracy}%\")\n",
    "    return accuracy\n",
    "\n",
    "def extract_metrics_from_log(log_file):\n",
    "    eval_loss = None\n",
    "    eval_acc = None\n",
    "    timestamp = None\n",
    "    \n",
    "    with open(log_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            if \"eval_loss =\" in line:\n",
    "                timestamp = line.split(\" - \")[0]\n",
    "                eval_loss = float(line.split(\"eval_loss =\")[1].strip())\n",
    "            elif \"eval_acc =\" in line:\n",
    "                eval_acc = float(line.split(\"eval_acc =\")[1].strip())\n",
    "    \n",
    "    return timestamp, eval_loss, eval_acc\n",
    "\n",
    "def save_client_metrics(client_id, round_num, log_file):\n",
    "    timestamp, eval_loss, eval_acc = extract_metrics_from_log(log_file)\n",
    "    if eval_loss is not None and eval_acc is not None and timestamp is not None:\n",
    "        filename = f\"client_{client_id}.txt\"\n",
    "        with open(filename, 'a') as f:\n",
    "            f.write(f\"{timestamp} - INFO - __main__ -     eval_loss = {eval_loss}\\n\")\n",
    "            f.write(f\"{timestamp} - INFO - __main__ -     eval_acc = {eval_acc}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1aa85a6e-b82e-4eb8-aea0-08d689407d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\t     mezo_round_1_client_1.log\tresult\t\tsteps_seeds.csv\n",
      "finetune.sh  mezo_round_2_client_1.log\trun.py\t\ttools\n",
      "log\t     mezo_round_3_client_1.log\trun_fewshot.sh\n",
      "log.lock     mezo_round_4_client_1.log\tsaved_model\n",
      "mezo.sh      mezo_round_5_client_1.log\tsrc\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a4c504cf-f73c-4920-8d34-a90fd3d896de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Round 1\n",
      "------------------------------------Executing command for Client 1 in Round 1:\n",
      "Deleted: data/k-shot-1k-test/SST-2/128-42/cached_train_RobertaTokenizerFast-roberta_128_sst-2\n",
      "Deleted: data/k-shot-1k-test/SST-2/128-42/cached_test_RobertaTokenizerFast-roberta_128_sst-2.lock\n",
      "Deleted: data/k-shot-1k-test/SST-2/128-42/cached_dev_RobertaTokenizerFast-roberta_128_sst-2.lock\n",
      "Deleted: data/k-shot-1k-test/SST-2/128-42/cached_train_RobertaTokenizerFast-roberta_128_sst-2.lock\n",
      "Deleted: data/k-shot-1k-test/SST-2/128-42/cached_test_RobertaTokenizerFast-roberta_128_sst-2\n",
      "Deleted: data/k-shot-1k-test/SST-2/128-42/cached_dev_RobertaTokenizerFast-roberta_128_sst-2\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>USING BP Free\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "By default for RoBERTa models the input embeddings and the output embeddings are NOT tied!!!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"output_model\",\n",
      "  \"apply_lora\": false,\n",
      "  \"architectures\": [\n",
      "    \"RobertaModelForPromptFinetuning\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"lora_alpha\": null,\n",
      "  \"lora_r\": null,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Averaging weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "By default for RoBERTa models the input embeddings and the output embeddings are NOT tied!!!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"apply_lora\": false,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"lora_alpha\": null,\n",
      "  \"lora_r\": null,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModelForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.embeddings.position_ids', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global model saved to saved_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "By default for RoBERTa models the input embeddings and the output embeddings are NOT tied!!!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"saved_model\",\n",
      "  \"apply_lora\": false,\n",
      "  \"architectures\": [\n",
      "    \"RobertaModelForPromptFinetuning\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"lora_alpha\": null,\n",
      "  \"lora_r\": null,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Text: This movie is great! It was <mask>.\n",
      "Input IDs: tensor([[    0,   713,  1569,    16,   372,   328,    85,    21, 50264,     4,\n",
      "             2]], device='cuda:0')\n",
      "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "Mask Position: 8\n",
      "Logits Shape: torch.Size([1, 2])\n",
      "Corrected Logits Shape: torch.Size([1, 11, 50265])\n",
      "Predicted: positive, Expected: positive\n",
      "Logits - great: 59.91145324707031, terrible: 55.79402160644531\n",
      "Text: I hated it. It was <mask>.\n",
      "Input IDs: tensor([[    0,   100, 19975,    24,     4,    85,    21, 50264,     4,     2]],\n",
      "       device='cuda:0')\n",
      "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "Mask Position: 7\n",
      "Logits Shape: torch.Size([1, 2])\n",
      "Corrected Logits Shape: torch.Size([1, 10, 50265])\n",
      "Predicted: negative, Expected: negative\n",
      "Logits - great: 55.99366760253906, terrible: 59.89561080932617\n",
      "Evaluation accuracy for saved_model: 100.0%\n",
      "Starting Round 2\n",
      "------------------------------------Executing command for Client 1 in Round 2:\n",
      "Deleted: data/k-shot-1k-test/SST-2/128-42/cached_train_RobertaTokenizerFast-roberta_128_sst-2\n",
      "Deleted: data/k-shot-1k-test/SST-2/128-42/cached_test_RobertaTokenizerFast-roberta_128_sst-2.lock\n",
      "Deleted: data/k-shot-1k-test/SST-2/128-42/cached_dev_RobertaTokenizerFast-roberta_128_sst-2.lock\n",
      "Deleted: data/k-shot-1k-test/SST-2/128-42/cached_train_RobertaTokenizerFast-roberta_128_sst-2.lock\n",
      "Deleted: data/k-shot-1k-test/SST-2/128-42/cached_test_RobertaTokenizerFast-roberta_128_sst-2\n",
      "Deleted: data/k-shot-1k-test/SST-2/128-42/cached_dev_RobertaTokenizerFast-roberta_128_sst-2\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>USING BP Free\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "By default for RoBERTa models the input embeddings and the output embeddings are NOT tied!!!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"output_model\",\n",
      "  \"apply_lora\": false,\n",
      "  \"architectures\": [\n",
      "    \"RobertaModelForPromptFinetuning\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"lora_alpha\": null,\n",
      "  \"lora_r\": null,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Averaging weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "By default for RoBERTa models the input embeddings and the output embeddings are NOT tied!!!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"saved_model\",\n",
      "  \"apply_lora\": false,\n",
      "  \"architectures\": [\n",
      "    \"RobertaModelForPromptFinetuning\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"lora_alpha\": null,\n",
      "  \"lora_r\": null,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Global model saved to saved_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "By default for RoBERTa models the input embeddings and the output embeddings are NOT tied!!!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"saved_model\",\n",
      "  \"apply_lora\": false,\n",
      "  \"architectures\": [\n",
      "    \"RobertaModelForPromptFinetuning\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"lora_alpha\": null,\n",
      "  \"lora_r\": null,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Text: This movie is great! It was <mask>.\n",
      "Input IDs: tensor([[    0,   713,  1569,    16,   372,   328,    85,    21, 50264,     4,\n",
      "             2]], device='cuda:0')\n",
      "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "Mask Position: 8\n",
      "Logits Shape: torch.Size([1, 2])\n",
      "Corrected Logits Shape: torch.Size([1, 11, 50265])\n",
      "Predicted: positive, Expected: positive\n",
      "Logits - great: 59.93278503417969, terrible: 55.82590103149414\n",
      "Text: I hated it. It was <mask>.\n",
      "Input IDs: tensor([[    0,   100, 19975,    24,     4,    85,    21, 50264,     4,     2]],\n",
      "       device='cuda:0')\n",
      "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "Mask Position: 7\n",
      "Logits Shape: torch.Size([1, 2])\n",
      "Corrected Logits Shape: torch.Size([1, 10, 50265])\n",
      "Predicted: negative, Expected: negative\n",
      "Logits - great: 55.95145034790039, terrible: 59.87339401245117\n",
      "Evaluation accuracy for saved_model: 100.0%\n",
      "Starting Round 3\n",
      "------------------------------------Executing command for Client 1 in Round 3:\n",
      "Deleted: data/k-shot-1k-test/SST-2/128-42/cached_train_RobertaTokenizerFast-roberta_128_sst-2\n",
      "Deleted: data/k-shot-1k-test/SST-2/128-42/cached_test_RobertaTokenizerFast-roberta_128_sst-2.lock\n",
      "Deleted: data/k-shot-1k-test/SST-2/128-42/cached_dev_RobertaTokenizerFast-roberta_128_sst-2.lock\n",
      "Deleted: data/k-shot-1k-test/SST-2/128-42/cached_train_RobertaTokenizerFast-roberta_128_sst-2.lock\n",
      "Deleted: data/k-shot-1k-test/SST-2/128-42/cached_test_RobertaTokenizerFast-roberta_128_sst-2\n",
      "Deleted: data/k-shot-1k-test/SST-2/128-42/cached_dev_RobertaTokenizerFast-roberta_128_sst-2\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>USING BP Free\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "By default for RoBERTa models the input embeddings and the output embeddings are NOT tied!!!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"output_model\",\n",
      "  \"apply_lora\": false,\n",
      "  \"architectures\": [\n",
      "    \"RobertaModelForPromptFinetuning\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"lora_alpha\": null,\n",
      "  \"lora_r\": null,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Averaging weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "By default for RoBERTa models the input embeddings and the output embeddings are NOT tied!!!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"saved_model\",\n",
      "  \"apply_lora\": false,\n",
      "  \"architectures\": [\n",
      "    \"RobertaModelForPromptFinetuning\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"lora_alpha\": null,\n",
      "  \"lora_r\": null,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Global model saved to saved_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "By default for RoBERTa models the input embeddings and the output embeddings are NOT tied!!!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"saved_model\",\n",
      "  \"apply_lora\": false,\n",
      "  \"architectures\": [\n",
      "    \"RobertaModelForPromptFinetuning\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"lora_alpha\": null,\n",
      "  \"lora_r\": null,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Text: This movie is great! It was <mask>.\n",
      "Input IDs: tensor([[    0,   713,  1569,    16,   372,   328,    85,    21, 50264,     4,\n",
      "             2]], device='cuda:0')\n",
      "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "Mask Position: 8\n",
      "Logits Shape: torch.Size([1, 2])\n",
      "Corrected Logits Shape: torch.Size([1, 11, 50265])\n",
      "Predicted: positive, Expected: positive\n",
      "Logits - great: 59.90009689331055, terrible: 55.81465148925781\n",
      "Text: I hated it. It was <mask>.\n",
      "Input IDs: tensor([[    0,   100, 19975,    24,     4,    85,    21, 50264,     4,     2]],\n",
      "       device='cuda:0')\n",
      "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "Mask Position: 7\n",
      "Logits Shape: torch.Size([1, 2])\n",
      "Corrected Logits Shape: torch.Size([1, 10, 50265])\n",
      "Predicted: negative, Expected: negative\n",
      "Logits - great: 55.95223617553711, terrible: 59.875057220458984\n",
      "Evaluation accuracy for saved_model: 100.0%\n",
      "Starting Round 4\n",
      "------------------------------------Executing command for Client 1 in Round 4:\n",
      "Deleted: data/k-shot-1k-test/SST-2/128-42/cached_train_RobertaTokenizerFast-roberta_128_sst-2\n",
      "Deleted: data/k-shot-1k-test/SST-2/128-42/cached_test_RobertaTokenizerFast-roberta_128_sst-2.lock\n",
      "Deleted: data/k-shot-1k-test/SST-2/128-42/cached_dev_RobertaTokenizerFast-roberta_128_sst-2.lock\n",
      "Deleted: data/k-shot-1k-test/SST-2/128-42/cached_train_RobertaTokenizerFast-roberta_128_sst-2.lock\n",
      "Deleted: data/k-shot-1k-test/SST-2/128-42/cached_test_RobertaTokenizerFast-roberta_128_sst-2\n",
      "Deleted: data/k-shot-1k-test/SST-2/128-42/cached_dev_RobertaTokenizerFast-roberta_128_sst-2\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>USING BP Free\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "By default for RoBERTa models the input embeddings and the output embeddings are NOT tied!!!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"output_model\",\n",
      "  \"apply_lora\": false,\n",
      "  \"architectures\": [\n",
      "    \"RobertaModelForPromptFinetuning\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"lora_alpha\": null,\n",
      "  \"lora_r\": null,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Averaging weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "By default for RoBERTa models the input embeddings and the output embeddings are NOT tied!!!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"saved_model\",\n",
      "  \"apply_lora\": false,\n",
      "  \"architectures\": [\n",
      "    \"RobertaModelForPromptFinetuning\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"lora_alpha\": null,\n",
      "  \"lora_r\": null,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Global model saved to saved_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "By default for RoBERTa models the input embeddings and the output embeddings are NOT tied!!!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"saved_model\",\n",
      "  \"apply_lora\": false,\n",
      "  \"architectures\": [\n",
      "    \"RobertaModelForPromptFinetuning\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"lora_alpha\": null,\n",
      "  \"lora_r\": null,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Text: This movie is great! It was <mask>.\n",
      "Input IDs: tensor([[    0,   713,  1569,    16,   372,   328,    85,    21, 50264,     4,\n",
      "             2]], device='cuda:0')\n",
      "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "Mask Position: 8\n",
      "Logits Shape: torch.Size([1, 2])\n",
      "Corrected Logits Shape: torch.Size([1, 11, 50265])\n",
      "Predicted: positive, Expected: positive\n",
      "Logits - great: 59.875308990478516, terrible: 55.804325103759766\n",
      "Text: I hated it. It was <mask>.\n",
      "Input IDs: tensor([[    0,   100, 19975,    24,     4,    85,    21, 50264,     4,     2]],\n",
      "       device='cuda:0')\n",
      "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "Mask Position: 7\n",
      "Logits Shape: torch.Size([1, 2])\n",
      "Corrected Logits Shape: torch.Size([1, 10, 50265])\n",
      "Predicted: negative, Expected: negative\n",
      "Logits - great: 55.94929122924805, terrible: 59.888343811035156\n",
      "Evaluation accuracy for saved_model: 100.0%\n",
      "Starting Round 5\n",
      "------------------------------------Executing command for Client 1 in Round 5:\n",
      "Deleted: data/k-shot-1k-test/SST-2/128-42/cached_train_RobertaTokenizerFast-roberta_128_sst-2\n",
      "Deleted: data/k-shot-1k-test/SST-2/128-42/cached_test_RobertaTokenizerFast-roberta_128_sst-2.lock\n",
      "Deleted: data/k-shot-1k-test/SST-2/128-42/cached_dev_RobertaTokenizerFast-roberta_128_sst-2.lock\n",
      "Deleted: data/k-shot-1k-test/SST-2/128-42/cached_train_RobertaTokenizerFast-roberta_128_sst-2.lock\n",
      "Deleted: data/k-shot-1k-test/SST-2/128-42/cached_test_RobertaTokenizerFast-roberta_128_sst-2\n",
      "Deleted: data/k-shot-1k-test/SST-2/128-42/cached_dev_RobertaTokenizerFast-roberta_128_sst-2\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>USING BP Free\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "By default for RoBERTa models the input embeddings and the output embeddings are NOT tied!!!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"output_model\",\n",
      "  \"apply_lora\": false,\n",
      "  \"architectures\": [\n",
      "    \"RobertaModelForPromptFinetuning\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"lora_alpha\": null,\n",
      "  \"lora_r\": null,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Averaging weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "By default for RoBERTa models the input embeddings and the output embeddings are NOT tied!!!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"saved_model\",\n",
      "  \"apply_lora\": false,\n",
      "  \"architectures\": [\n",
      "    \"RobertaModelForPromptFinetuning\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"lora_alpha\": null,\n",
      "  \"lora_r\": null,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Global model saved to saved_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "By default for RoBERTa models the input embeddings and the output embeddings are NOT tied!!!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"saved_model\",\n",
      "  \"apply_lora\": false,\n",
      "  \"architectures\": [\n",
      "    \"RobertaModelForPromptFinetuning\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"lora_alpha\": null,\n",
      "  \"lora_r\": null,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Text: This movie is great! It was <mask>.\n",
      "Input IDs: tensor([[    0,   713,  1569,    16,   372,   328,    85,    21, 50264,     4,\n",
      "             2]], device='cuda:0')\n",
      "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "Mask Position: 8\n",
      "Logits Shape: torch.Size([1, 2])\n",
      "Corrected Logits Shape: torch.Size([1, 11, 50265])\n",
      "Predicted: positive, Expected: positive\n",
      "Logits - great: 59.88344192504883, terrible: 55.79084396362305\n",
      "Text: I hated it. It was <mask>.\n",
      "Input IDs: tensor([[    0,   100, 19975,    24,     4,    85,    21, 50264,     4,     2]],\n",
      "       device='cuda:0')\n",
      "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "Mask Position: 7\n",
      "Logits Shape: torch.Size([1, 2])\n",
      "Corrected Logits Shape: torch.Size([1, 10, 50265])\n",
      "Predicted: negative, Expected: negative\n",
      "Logits - great: 55.910400390625, terrible: 59.873443603515625\n",
      "Evaluation accuracy for saved_model: 100.0%\n",
      "All rounds completed.\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class DynamicTrainingArguments:\n",
    "    sfc: bool = False\n",
    "    use_task_word: bool = False\n",
    "    num_labels: int = 2\n",
    "    binary_classification: bool = True\n",
    "    ub: float = 1.0\n",
    "    lb: float = 0.0\n",
    "\n",
    "try:\n",
    "    from src.models import RobertaModelForPromptFinetuning\n",
    "except ImportError:\n",
    "    from transformers import RobertaForSequenceClassification as RobertaModelForPromptFinetuning\n",
    "    print(\"Warning: Using fallback model.\")\n",
    "\n",
    "def main():\n",
    "    num_rounds = 5\n",
    "    clients = 1\n",
    "    parent_folder = \"output_model\"\n",
    "    global_model_output_dir = \"saved_model\"\n",
    "    k = 128\n",
    "    seed = 42\n",
    "    \n",
    "    #bp as 0, bp free as 1\n",
    "    status = [1]\n",
    "    \n",
    "    # Clear existing client files\n",
    "    for client_id in range(1, clients + 1):\n",
    "        filename = f\"client_{client_id}.txt\"\n",
    "        if os.path.exists(filename):\n",
    "            os.remove(filename)\n",
    "\n",
    "    for round_num in range(1, num_rounds + 1):\n",
    "        print(f\"Starting Round {round_num}\")\n",
    "        averaged_state_dict = {}\n",
    "\n",
    "        for client_id in range(1, clients + 1):\n",
    "            print(f\"------------------------------------Executing command for Client {client_id} in Round {round_num}:\")\n",
    "            directory = f\"data/k-shot-1k-test/SST-2/{k}-{seed}/\"\n",
    "            cached_files = glob.glob(os.path.join(directory, \"cached*\"))\n",
    "            for file_path in cached_files:\n",
    "                try:\n",
    "                    os.remove(file_path)\n",
    "                    print(f\"Deleted: {file_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error deleting {file_path}: {e}\")\n",
    "\n",
    "            command = f\"mv data/k-shot-1k-test/SST-2/{k}-{seed}/train_part{client_id}.tsv data/k-shot-1k-test/SST-2/{k}-{seed}/train.tsv\"\n",
    "            os.system(command)\n",
    "\n",
    "            log_file = f\"mezo_round_{round_num}_client_{client_id}.log\"\n",
    "            if status[client_id-1] == 0:\n",
    "                print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>USING BP\")\n",
    "                if round_num == 1:\n",
    "                    command = f\"WANDB_MODE=offline TASK=SST-2 K={k} SEED={seed} BS=64 LR=1e-6 STEP=10 EVAL_STEP=10 MODEL=roberta-large bash mezo.sh\"\n",
    "                    # os.system(command)\n",
    "                else:\n",
    "                    command = f\"WANDB_MODE=offline TASK=SST-2 K={k} SEED={seed} BS=64 LR=1e-6 STEP=10 EVAL_STEP=10 MODEL={global_model_output_dir} bash mezo.sh\"\n",
    "                    # os.system(command)\n",
    "            \n",
    "            if status[client_id-1] == 1:\n",
    "                print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>USING BP Free\")\n",
    "                if round_num == 1:\n",
    "                    command = f\"WANDB_MODE=offline TASK=SST-2 K={k} SEED={seed} BS=64 LR=1e-6 STEP=10 EVAL_STEP=10 MODEL=roberta-large bash mezo.sh\"\n",
    "                    # os.system(command)\n",
    "                else:\n",
    "                    command = f\"WANDB_MODE=offline TASK=SST-2 K={k} SEED={seed} BS=64 LR=1e-6 STEP=10 EVAL_STEP=10 MODEL={global_model_output_dir} bash mezo.sh\"\n",
    "                    # os.system(command)\n",
    "            \n",
    "            with open(log_file, \"w\") as f:\n",
    "                process = subprocess.run(command, shell=True, stdout=f, stderr=subprocess.STDOUT, text=True)\n",
    "            # with open(log_file, \"r\") as f:\n",
    "            #     print(f\"MeZO stdout:\\n{f.read()}\")\n",
    "            if process.returncode != 0:\n",
    "                raise RuntimeError(\"MeZO execution failed\")\n",
    "\n",
    "            # Save metrics from log\n",
    "            save_client_metrics(client_id, round_num, log_file)\n",
    "\n",
    "            command = f\"mv data/k-shot-1k-test/SST-2/{k}-{seed}/train.tsv data/k-shot-1k-test/SST-2/{k}-{seed}/train_part{client_id}.tsv\"\n",
    "            os.system(command)\n",
    "\n",
    "            model = RobertaModelForPromptFinetuning.from_pretrained(parent_folder)\n",
    "            model.model_args = DynamicTrainingArguments()\n",
    "            tokenizer = RobertaTokenizer.from_pretrained(parent_folder)\n",
    "            state_dict = model.state_dict()\n",
    "\n",
    "            if not averaged_state_dict:\n",
    "                averaged_state_dict = {k: v.clone() for k, v in state_dict.items()}\n",
    "            else:\n",
    "                for key in state_dict.keys():\n",
    "                    if state_dict[key].is_floating_point():\n",
    "                        averaged_state_dict[key] += state_dict[key]\n",
    "                    else:\n",
    "                        averaged_state_dict[key] = state_dict[key].clone()\n",
    "\n",
    "            os.system(f\"rm -rf {parent_folder}\")\n",
    "\n",
    "        print(\"Averaging weights\")\n",
    "        for key in averaged_state_dict.keys():\n",
    "            if averaged_state_dict[key].is_floating_point():\n",
    "                averaged_state_dict[key] /= clients\n",
    "\n",
    "        if round_num == 1:\n",
    "            global_model = RobertaModelForPromptFinetuning.from_pretrained('roberta-large')\n",
    "        else:\n",
    "            global_model = RobertaModelForPromptFinetuning.from_pretrained(global_model_output_dir)\n",
    "        global_model.model_args = DynamicTrainingArguments()\n",
    "        global_model.load_state_dict(averaged_state_dict)\n",
    "        global_model.save_pretrained(global_model_output_dir)\n",
    "        tokenizer.save_pretrained(global_model_output_dir)\n",
    "        print(f\"Global model saved to {global_model_output_dir}\")\n",
    "\n",
    "        evaluate_model(global_model_output_dir)\n",
    "        \n",
    "        torch.cuda.empty_cache()  # Clear the GPU cache\n",
    "        gc.collect() \n",
    "\n",
    "    print(\"All rounds completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70863d1a-5e3e-4d65-b03b-471c7b4a02ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4db3256-92e6-465e-8c62-e2b298941c04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
